[
  {
    "objectID": "tuesday2.html",
    "href": "tuesday2.html",
    "title": "National Park Species",
    "section": "",
    "text": "This analysis uses data from the October 8, 2024 TidyTuesday.\nThis is a dataset of species in American national parks, specifically the 15 most visited. The data is sourced from NPSpecies, a tool maintained by the National Park Service (NPS)’s Integrated Resource Management Applications (IRMA). It includes taxonomy information and other characteristics like sensitivities and rarity for each species found in each park.\n\nspecies &lt;- species |&gt;\n  filter(Family == 'Mustelidae') |&gt;\n  mutate(ParkName = str_replace(ParkName, ' National Park', '')) |&gt;\n  mutate(ParkName = fct_infreq(ParkName))\n\nspecies |&gt;\n  ggplot() +\n  geom_bar(aes(y = ParkName)) +\n  labs(\n    x = 'National Park',\n    y = '# Species',\n    title = 'Distinct species in family Mustelidae 15 national parks'\n  )\n\n\n\n\n\n\n\n\nThis data displays the number of mustelid (weasels, badgers, otters) species in each of the 15 most visited national parks. This is because mustelids are my favorite taxon. I would have included my second-favorite taxon, pinnipeds (seals, sea lions, walruses), however I discovered there are none in any of the listed parks. The only conclusion I draw from this is that more people should visit Alaskan national parks.\n\n\n# A tibble: 15 × 2\n   ParkName              count\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Glacier                   9\n 2 Grand Teton               9\n 3 Yellowstone               8\n 4 Yosemite                  8\n 5 Acadia                    7\n 6 Great Smoky Mountains     7\n 7 Olympic                   7\n 8 Rocky Mountain            7\n 9 Indiana Dunes             6\n10 Cuyahoga Valley           5\n11 Grand Canyon              4\n12 Bryce Canyon              3\n13 Hot Springs               3\n14 Zion                      3\n15 Joshua Tree               2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nikhil Shah",
    "section": "",
    "text": "My name is Nikhil Shah and I’m an upcoming senior at Pitzer College majoring in Environmental Science and Policy, with a minor in Data Science. This website currently contains a collection of data science mini-projects, and will soon be updated to include larger projects and other content."
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "SQL: Hungry Cop Effect",
    "section": "",
    "text": "It’s been observed that judges are less likely to grant lenient sentences or parole when they are hungry. (i.e. before their lunch break) (Danziger et al., 2011). This has become known as the “hungry judge effect”.\nWe can use data from the Stanford Open Policing Project (Pierson et al., 2020) to observe the rate at which traffic stops result in citations, and whether there’s a meaningful difference in this rate before and after noon (which, presumably, is roughly when cops would eat lunch and be the hungriest). Washington, Texas, and New Jersey all give data for citations given and time of day.\nI cut off the data according to the normal 9-5 work day. While traffic stops obviously happen outside of those hours, it doesn’t make sense to evaluate the effect of a lunch break on something happening in the middle of the night."
  },
  {
    "objectID": "project5.html#introduction",
    "href": "project5.html#introduction",
    "title": "SQL: Hungry Cop Effect",
    "section": "",
    "text": "It’s been observed that judges are less likely to grant lenient sentences or parole when they are hungry. (i.e. before their lunch break) (Danziger et al., 2011). This has become known as the “hungry judge effect”.\nWe can use data from the Stanford Open Policing Project (Pierson et al., 2020) to observe the rate at which traffic stops result in citations, and whether there’s a meaningful difference in this rate before and after noon (which, presumably, is roughly when cops would eat lunch and be the hungriest). Washington, Texas, and New Jersey all give data for citations given and time of day.\nI cut off the data according to the normal 9-5 work day. While traffic stops obviously happen outside of those hours, it doesn’t make sense to evaluate the effect of a lunch break on something happening in the middle of the night."
  },
  {
    "objectID": "project5.html#data",
    "href": "project5.html#data",
    "title": "SQL: Hungry Cop Effect",
    "section": "Data",
    "text": "Data\n\nLibraries\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\n\n\nSQL Connection\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\n\nQuery\n\nSELECT 'WA' AS state,\n       CASE\n        WHEN time &lt; '12:00:00' THEN 'morning'\n        ELSE 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate\nFROM wa_statewide_2020_04_01\nWHERE time &gt; '09:00:00' AND time &lt; '17:00:00'\nGROUP BY day_segment\n\nUNION ALL\n\nSELECT 'TX' AS state,\n       CASE\n        WHEN time &lt; '12:00:00' THEN 'morning'\n        ELSE 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate\nFROM tx_statewide_2020_04_01\nWHERE time &gt; '09:00:00' AND time &lt; '17:00:00'\nGROUP BY day_segment\n\nUNION ALL\n\nSELECT 'NJ' AS state,\n       CASE\n        WHEN time &lt; '12:00:00' THEN 'morning'\n        ELSE 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate\nFROM nj_statewide_2020_04_01\nWHERE time &gt; '09:00:00' AND time &lt; '17:00:00'\nGROUP BY day_segment;\n\nThis takes the overall data for a state, filters out times outside of the normal workday, adds a column that specifies what part of the day a stop occurred at based on the time, and splits the table into two parts for each part of the day. It averages the column “citation_issued”, which is 1s for yes and 0s for no, to get the rate at which citations were issued at each part of the day. It combines this output for three different states into one table.\n\ncitations_and_warnings\n\n  state day_segment citation_rate warning_rate\n1    WA   afternoon     0.3797482    0.4209331\n2    WA     morning     0.4442537    0.4271534\n3    TX   afternoon     0.3883023    0.7490462\n4    TX     morning     0.3767222    0.7539040\n5    NJ   afternoon     0.6136698    0.4608858\n6    NJ     morning     0.6276607    0.4499691\n\n\nThe final data from the SQL query shows one row for each pairing of state and part of the day, and the proportion of traffic stops resulting in a citation or a warning in each."
  },
  {
    "objectID": "project5.html#visualization",
    "href": "project5.html#visualization",
    "title": "SQL: Hungry Cop Effect",
    "section": "Visualization",
    "text": "Visualization\n\ncitations_and_warnings |&gt;\n  ggplot() +\n  geom_col(\n    aes(x = state, y = citation_rate, fill = day_segment),\n    position = 'dodge'\n  ) +\n  labs(\n    x = 'State',\n    fill = 'Part of Day',\n    y = 'Citation Rate',\n    title = 'Frequency of Citations from Traffic Stops at Different Times of Day'\n  )\n\n\n\n\n\n\n\ncitations_and_warnings |&gt;\n  ggplot() +\n  geom_col(\n    aes(x = state, y = warning_rate, fill = day_segment),\n    position = 'dodge') +\n  labs(\n    x = 'State',\n    fill = 'Part of Day',\n    y = 'Warning Rate',\n    title = 'Frequency of Warnings from Traffic Stops at Different Times of Day'\n  )"
  },
  {
    "objectID": "project5.html#interpretation",
    "href": "project5.html#interpretation",
    "title": "SQL: Hungry Cop Effect",
    "section": "Interpretation",
    "text": "Interpretation\nThe answer seems to be a solid no. While 44% of morning traffic stops in Washington state result in citations, compared to 38% of afternoon ones, this pattern doesn’t exist in the other two states. The rate at which warnings are given is identical.\nNotably, without the time constraint, the morning citation rates are considerably higher than the afternoon ones for all three states. I assume this is because “morning” encompasses late-night traffic stops, which evidently result in much more citations. This proves that removing those parts of the data makes sense, because, as stated earlier, a DUI citation at 3 am probably doesn’t have any degree of dependence on whether the officer has had lunch yet.\nOverall, the null hypothesis seems to be supported in this case. Big states like California and New Jersey do not supply time of citation information, so it may be helpful to collect more data to confirm these findings."
  },
  {
    "objectID": "project5.html#references",
    "href": "project5.html#references",
    "title": "SQL: Hungry Cop Effect",
    "section": "References",
    "text": "References\n\nDanziger, Shal, Jonathan Levav, Liora Avnaim-Pesso (2011). “Extraneous Factors in Judicial Decisions.” Proc. Natl. Acad. Sci. U.S.A., 108 (17) 6889-6892.\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. (2020). “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Biased Hiring",
    "section": "",
    "text": "Introduction\nIn the mid-2010s, Amazon developed an AI-based tool trained to evaluate resumes to output the best possible candidates without the need for human review. This project failed, because it turned out the training data and the output was biased against women.\n\n\nAnalysis\nThis failure stems from the lack of adherence to one of the principles of data science, which suggests to “build teams with diverse ideas, backgrounds, and strengths.” According to the ACLU, Amazon’s workforce was predominantly male, which led to the subset of “successful resumes” being predominantly male, so the algorithm learned that characteristics that could have been associated with women were not favorable. Notably, the Reuters report claims that the bias was primarily due to disproportionate gender ratios in the applications themselves, rather than the existing workforce, but it’s fair to assume that a skewed workforce would perpetuate that ratio and so the premise is essentially the same - a lack of diversity in Amazon led to a discriminatory hiring algorithm.\nAnother principle for which the project falls short is that it does not use data to improve lives. In fact, pioneering a discriminatory technology that would go on to be implemented on massive scales ten years later, governing a major factor in peoples’ quality of life (their employment), most likely played a significant role in making lives worse, even if the algorithm itself was scrapped. In the 1970s, IBM stated that “a computer can never be held accountable, therefore a computer must never make a management decision.” While collecting data for the sake of knowledege is okay, a machine-learning algorithm should not be given the power to have a tangible negative effect on human lives. Going through with this project without taking great care to prevent bias in the algorithm’s training data and thus its output may have contributed to this ideal being violated.\nI think it’s generally okay for gender to be used as a variable for data analysis purposes specifically. When collecting data on people, and that data is actionable, such as data collected on social media platforms for targeted advertising, or data collected by companies for hiring, collecting data on social categories like gender can very easily be intrusive and unethical, with the potential to be misused. However, in studies, especially when there’s a potential connection between those social categories and another variable, it can be very important to collect that data. If gender data had not been collected in this case, there would be no way of knowing about the discrimination that was occurring. Overall, the distinction probably lies in whether gender as a variable is pertinent to the project, and whether that data can be used to cause harm to the participants.\nData collection and processing in this case was most likely not conducted ethically. While the training data could have been gathered consensually (since it was from employees of the company), the project was conducted behind closed doors; actual applicants weren’t aware that their applications were being evaluated by a machine-learning algorithm. They had no way of knowing that their information was being used in a computer program which could discriminate against them for factors out of their control. On a few applications for jobs and internships, I’ve been asked whether or not I’d like to opt out of an AI evaluation of my application. While I did opt out, I imagine some or most other companies aren’t giving me the option because that was their default option. This worries me - how do I know my resume isn’t being used to train ChatGPT to hire people based on some immutable characteristic? Not only would this project have been unethical, it could have set a standard that green-lighted the same unethical behavior for broad use in the future.\n\n\nSources\n\nDastin, Jeffrey (2018). “Amazon scraps secret AI recruiting tool that showed bias against women.” Reuters.\nGoodman, Rachel (2018). “Why Amazon’s Automated Hiring Tool Discriminated Against Women.” ACLU."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Palindromes",
    "section": "",
    "text": "Data\nThe dataset is English Words by dwyl (Github). It contains almost all of the words in the English dictionary.\n\nlibrary(tidyverse)\nlibrary(stringi)\n\nwords &lt;- read.csv(\"data/words.csv\")\n\nIt also contains many things that are decidedly not words.\n\n\n\ncategory\nexample\nthing to filter out\n\n\n\n\nhas number\n1080\ndigits\n\n\nhas symbol\nA&P\nsymbols\n\n\nproper noun\nAbdul\ncapitals\n\n\ninitialism\nA.I.\ncapitals, periods\n\n\ncontractions\naren’t\napostrophes\n\n\npossessives\nabbot’s\napostrophes\n\n\ncompounds\nable-bodied\nhyphens\n\n\n\nI ended up filtering out any word that doesn’t have a vowel from the list of palindromes as well. While there are some vowel-less words that are valid words, it’s hard to argue that “m” or “tgt” or “xxx” are among them.\n\nwords_filtered &lt;- words |&gt;\n  rename('word' = X2) |&gt;\n  filter(!str_detect(word, '[[:upper:][:punct:]\\\\d]')) |&gt;\n  mutate(length = str_length(word)) |&gt;\n  mutate(start= str_sub(word, 1, 1))\n  \nsample_n(words_filtered, 10)\n\n            word length start\n1        sporran      7     s\n2         betels      6     b\n3    uncathartic     11     u\n4      relevying      9     r\n5   dissentiency     12     d\n6  unconsociable     13     u\n7        caracal      7     c\n8        haysuck      7     h\n9        abyssal      7     a\n10    dispurpose     10     d\n\n\n\n\nPalindromes\nA palindrome is any word (or set of words) that is the same when the letters are reversed.\n\npalindromes &lt;- words_filtered |&gt;\n  filter(word == stri_reverse(word)) |&gt;\n  filter(str_detect(word, '[aeiouy]'))\n\nsample_n(palindromes, 10)\n\n      word length start\n1     oooo      4     o\n2      ese      3     e\n3      eme      3     e\n4      iii      3     i\n5     toot      4     t\n6     acca      4     a\n7  deedeed      7     d\n8        a      1     a\n9    deled      5     d\n10    goog      4     g\n\n\nA lot of palindromes are formed because of common suffixes. ‘Deified’, ‘reviver’, and ‘stats’ might be palindromes, but only because of a modifier that changes the root word. This shows how many words follow those structures.\n\npalindromes &lt;- palindromes |&gt;\n  mutate(suffix = ifelse(str_detect(word, \"\\\\b(de|re|ro|s)\\\\w+(ed|er|or|s)\\\\b\"), 'Yes', 'No'))\n\npalindromes |&gt;\n  ggplot(aes(fill = suffix, x = length)) +\n  geom_bar() +\n  labs(\n    x = 'Word Length',\n    y = '# of Palindromes',\n    title = 'Single-Word Palindromes in English'\n  )\n\nThere are only 118 palindromes in this dataset. Of these, there’s only one palindrome longer than seven letters, and 29 are only palindromes because of a suffix, and the root word wouldn’t be one at all. In total, there are about 30 of them that you might encounter in your day-to-day life, which is only 1% of 1% of the words in English. I hear about palindromes constantly, but it turns out most of them are actually just engineered phrases like “race car” or “a man, a plan, a canal, panama”.\nI think it’s interesting how there seems to be some kind of inclination towards odd-length words, that have a unique letter in the middle, like “civic” or “refer” or “tenet”.\n\nwords_filtered |&gt;\n  ggplot(aes(x = length)) +\n  geom_bar() +\n  labs(\n    x = 'Word Length',\n    y = '# of Words',\n    title = 'Length of English Words'\n  )\n\n\n\n\n\n\n\n\nI tend to think of a syllable as a pair of a consonant sound and a vowel sound, and it looks like there’s a slight amount more of even-length words than odd in English, so I’m a bit surprised to see such a preference for odd-length ones out of the palindromes.\n\n\nAnti-Palindromes, or Reduplications\nWhile trying to work out a way to find palindromes with stringr functions (apparently R does not have reverse indexing like Python does so this is either not possible or very convoluted) I accidentally came up with a list of “anti-palindromes”, where the back half of the word is also the front half of the word. If “toot” is a palindrome, “toto” is an anti-palindrome.\n\nreduplications &lt;- words_filtered |&gt; \n  filter(str_sub(word, 1, floor(str_length(word)/2)) == str_sub(word, -ceiling(str_length(word)/2), -1))\n\nwords_letters &lt;- words_filtered |&gt;\n  group_by(start) |&gt;\n  summarize(count_total = n()) |&gt;\n  mutate(\"All English Words\" = count_total/sum(count_total)*100)\n\nreduplication_letters &lt;- reduplications |&gt;\n  group_by(start) |&gt;\n  summarize(count_red = n()) |&gt;\n  mutate(\"Anti-Palindromes\" = count_red/sum(count_red)*100)\n\nletters &lt;- words_letters |&gt;\n  left_join(reduplication_letters)\n\nJoining with `by = join_by(start)`\n\nletters_long &lt;- letters |&gt;\n  pivot_longer(cols = c(\"All English Words\", \"Anti-Palindromes\"), names_to = 'dictionary', values_to = 'percent') \n\nletters_long |&gt;\n  ggplot(aes(fill = dictionary, x = start, y = percent)) +\n  geom_bar(position = 'dodge', stat = 'identity') +\n  labs(\n    x = 'Letter',\n    y = 'Frequency (Percentage of Overall Set)',\n    title = 'Starting Letters of Anti-Palindromes'\n  )\n\n\n\n\n\n\n\n\nInterestingly, a lot of these anti-palindromes, or reduplications, start with uncommon letters. This could because many of those letters are emphatic consonant sounds that sound more interesting when repeated. Examples include beriberi, yoyo, and pompom."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Reverse 3-0 Odds",
    "section": "",
    "text": "In a best-of-5 set, a reverse 3-0 occurs when one side loses the first two games, and then wins the next three to win a set. It’s an uncommon and extremely exciting (or heartbreaking) phenomenon, and it can seem to often much more likely or much less often than you would expect. I want to estimate how frequently this happens when a set starts 2-0, depending on the relative strength of both sides.\nThis needs some assumptions. Firstly, the odds of one player or team to win a game heavily depends on relative skill, the volatility of the game, and maybe luck. Without looking at data from a specific game, we can create odds that seem to make sense.\nWe also have to look at players/teams of a somewhat comparable skill. If you were watching a best-of-5 tournament match, and you saw one side go down 0-2, you would only wonder if they could win the match if they were capable of making it competitive. In a 1024-entrant bracket, odds are, the 1st seed will beat the 1024th seed, and the 512th seed, and the 256th seed handily in a 3-0. While those could technically be a reverse 3-0 after the first two games, it’s very unlikely and nobody would expect it to happen. We want to look at matches where some competitiveness is expected.\nWe can use a 16-player tournament, and assume that, in a vacuum, the odds of one side winning a game are the the ratio of the two teams’ seeds. For instance, seed 1 has a 1:2 chance of beating seed 2 (67%), seed 16 has a 16:1 chance of beating seed 1 (6%), and seed 15 has 15:16 odds of beating seed 16 (48%). Each of these matches would be competitive, with a significant degree of variability, which makes it interesting.\nLet’s create a hypothetical match between a random pairing of two seeds from 1-16. We can take one side’s odds of winning a game to the power of three and determine the chance of them winning three games in a row. However, since we want to know the chance of a reverse sweep rather than just a sweep, we need to account for the fact that the player or team that goes up 2-0 might have some additional advantage specifically related to that opponent: experience in the matchup, knowledge of habits or strategies, a favorable difference in skill sets, etc. Therefore, the other side’s chance of winning a game is most likely lower than initially assumed.\nHow can we account for this? If you flip a coin twice, 25% of the time, it’ll land on tails twice by pure random chance. In that spirit, let’s say there was a 25% chance that the 2-0 start was purely based on random variability, and reduce the losing side’s new odds of winning a game by 25%. Based on my general experience with spectating tournaments, this is a somewhat accurate measure.\n\nlibrary(tidyverse)\nlibrary(purrr)\n\n\n# Precursor for the upset_factor() function\nexpected_placement &lt;- function(x) {\n  if (x &gt;= 1 & x &lt;= 4) {\n    order &lt;- x\n  }\n  else if (x == 5 | x == 6) {\n    order &lt;- 5\n  }\n  else if (x ==7 | x == 8) {\n    order &lt;- 6\n  }\n  else if (x &gt;= 9 & x &lt;= 12) {\n    order &lt;- 7\n  }\n  else if (x &gt;= 13 & x &lt;= 16) {\n    order &lt;- 8\n  }\n}\n\n# This scales the difference in seeds according to the difference in their expected placements in a double elimination bracket. Seed 1 vs Seed 4 is a similar difference to Seed 5 vs Seed 15. \nupset_factor &lt;- function(x, y) {\n  upset &lt;- expected_placement(x) - expected_placement(y)\n}\n\n# Randomizes seeds of two opponents, determines the odds of the second team winning games after going down 0-2, finds the seed differential from the previous functions.\nreverse_sweep_chance &lt;- function() {\n  first_two_winner &lt;- sample(1:16, 1)\n  reverse_sweep_candidate &lt;- sample(1:16, 1)\n  win_odds &lt;- (first_two_winner / (first_two_winner + reverse_sweep_candidate) * 0.75)\n  did_they_win &lt;- (runif(1) &lt; win_odds * win_odds * win_odds)\n  seed_comparison &lt;- upset_factor(reverse_sweep_candidate, first_two_winner)\n  return(data.frame(reverse_sweep = did_they_win, upset_factor = seed_comparison))\n}\n\n# Repeats the test\ndata &lt;- map(1:1000, ~ reverse_sweep_chance()) |&gt;\n  list_rbind() \n\n# Finds the overall total of reverse sweeps that happened\ndata |&gt;\n  summarize(chance = mean(reverse_sweep))\n\n  chance\n1  0.085\n\n# Plots the data\ndata |&gt;\n  ggplot(aes(x = upset_factor, fill = reverse_sweep)) +\n  geom_bar(position = 'fill') +\n  labs(\n    x = 'Upset Factor',\n    y = 'Reverse 3-0 Chance',\n    fill = 'Did It Happen?',\n    title = 'Reverse Sweep Odds Depending on Seed Differential'\n  )\n\n\n\n\n\n\n\n\nI think this ended up being a very good estimation of the chances of this happening across different seeds. While relatively equally-skilled opponents can rarely make a reverse sweep, when the opponent that’s up 2-0 is marginally higher seeded it almost never happens. Meanwhile, when the lower seeded player goes up early, they still stand a very significant chance of getting reverse swept. This simulation largely aligns with my experience with competitive games and sports.\nThis simulation helps to illustrate how likely (or unlikely) a major comeback is, even with a high degree of volatility. Conversely, it shows how an early match lead in a best of 5 does not remotely guarantee victory against a better opponent. While the exact numbers definitely could be adjusted for more accurate results across different games, I think this does a good job of representing the probability of a reverse sweep depending on the strength of one’s opponent. Despite the fact that the odds of winning are completely made up, I do think the simulation lends credibility to the idea that the likelihood of this phenomenon can be extremely variable depending on the relative skill levels of two opponents."
  },
  {
    "objectID": "tuesday1.html",
    "href": "tuesday1.html",
    "title": "Lichess.org",
    "section": "",
    "text": "This analysis uses data from the October 1, 2024 TidyTuesday.\nThis is a dataset of 20,058 online chess games. The data was collected and cleaned from Lichess.org by Kaggle/Mitchell J. It includes extensive data on each game including each move used, both players’ usernames and ratings, and the time of the game.\n\n# adds a column that isolates the opening move of each game, and orders it by how common each move is\nchess &lt;- chess |&gt;\n  mutate(\n    opening_move = ifelse(\n      str_sub(moves, 1, 1)=='N', \n      str_sub(moves, 1, 3), \n      str_sub(moves, 1, 2)\n      )\n    ) |&gt;\n  mutate(\n    opening_move = fct_infreq(opening_move)\n    )\n\n# creates a bar plot that shows the frequency of opening moves\nchess |&gt;\n  ggplot() +\n  geom_bar(aes(x = opening_move)) +\n  labs(\n    x = 'Opening Move',\n    y = 'Number of Games',\n    title = 'Frequency of opening moves for white on Lichess.com'\n  )\n\n\n\n\n\n\n\n\nThis bar plot shows the frequency of initial moves in the dataset. e4 and d4 make up almost all of them. While knight to f3 is the third most popular, almost none of the games use knight to a3.\n\n\n# A tibble: 20 × 3\n   opening_move count proportion\n   &lt;fct&gt;        &lt;int&gt;      &lt;dbl&gt;\n 1 e4           12598    62.8   \n 2 d4            4522    22.5   \n 3 Nf3            725     3.61  \n 4 c4             716     3.57  \n 5 e3             416     2.07  \n 6 g3             186     0.927 \n 7 b3             173     0.862 \n 8 f4             166     0.828 \n 9 d3             131     0.653 \n10 Nc3             99     0.494 \n11 b4              88     0.439 \n12 c3              56     0.279 \n13 g4              38     0.189 \n14 h4              33     0.165 \n15 a4              28     0.140 \n16 a3              27     0.135 \n17 f3              23     0.115 \n18 Nh3             15     0.0748\n19 h3              14     0.0698\n20 Na3              4     0.0199"
  },
  {
    "objectID": "textanalysis.html",
    "href": "textanalysis.html",
    "title": "Palindromes",
    "section": "",
    "text": "The dataset is English Words by dwyl (Github). It contains almost all of the words in the English dictionary.\n\nlibrary(tidyverse)\nlibrary(stringi)\n\nwords &lt;- read.csv(\"data/words.csv\")\n\nIt also contains many things that are decidedly not words.\n\n\n\ncategory\nexample\nthing to filter out\n\n\n\n\nhas number\n1080\ndigits\n\n\nhas symbol\nA&P\nsymbols\n\n\nproper noun\nAbdul\ncapitals\n\n\ninitialism\nA.I.\ncapitals, periods\n\n\ncontractions\naren’t\napostrophes\n\n\npossessives\nabbot’s\napostrophes\n\n\ncompounds\nable-bodied\nhyphens\n\n\n\nI ended up filtering out any word that doesn’t have a vowel from the list of palindromes as well. While there are some vowel-less words that are valid words, it’s hard to argue that “m” or “tgt” or “xxx” are among them.\n\nwords_filtered &lt;- words |&gt;\n  rename('word' = X2) |&gt;\n  filter(!str_detect(word, '[[:upper:][:punct:]\\\\d]')) |&gt;\n  mutate(length = str_length(word)) |&gt;\n  mutate(start= str_sub(word, 1, 1))\n  \nsample_n(words_filtered, 10)\n\n              word length start\n1           achime      6     a\n2        macerater      9     m\n3  nonpromulgation     15     n\n4         nicotian      8     n\n5     unvoluptuous     12     u\n6      postfrontal     11     p\n7         niteries      8     n\n8     extralegally     12     e\n9            crash      5     c\n10          unclot      6     u"
  },
  {
    "objectID": "dataviz1.html",
    "href": "dataviz1.html",
    "title": "Lichess.org",
    "section": "",
    "text": "This analysis uses data from the October 1, 2024 TidyTuesday.\nThis is a dataset of 20,058 online chess games. The data was collected and cleaned from Lichess.org by Kaggle/Mitchell J. It includes extensive data on each game including each move used, both players’ usernames and ratings, and the time of the game.\n\n# adds a column that isolates the opening move of each game, and orders it by how common each move is\nchess &lt;- chess |&gt;\n  mutate(\n    opening_move = ifelse(\n      str_sub(moves, 1, 1)=='N', \n      str_sub(moves, 1, 3), \n      str_sub(moves, 1, 2)\n      )\n    ) |&gt;\n  mutate(\n    opening_move = fct_infreq(opening_move)\n    )\n\n# creates a bar plot that shows the frequency of opening moves\nchess |&gt;\n  ggplot() +\n  geom_bar(aes(x = opening_move)) +\n  labs(\n    x = 'Opening Move',\n    y = 'Number of Games',\n    title = 'Frequency of opening moves for white on Lichess.org',\n    alt = 'Bar chart showing the frequency of opening moves for white on Lichess.org, ordered by frequency. e4, d4, Nf3, c4 are the highest, the rest are largely negligible.'\n  )\n\n\n\n\n\n\n\n\nThis bar plot shows the frequency of initial moves in the dataset. e4 and d4 make up almost all of them. While knight to f3 is the third most popular, almost none of the games use knight to a3.\n\n\n# A tibble: 20 × 3\n   opening_move count proportion\n   &lt;fct&gt;        &lt;int&gt;      &lt;dbl&gt;\n 1 e4           12598    62.8   \n 2 d4            4522    22.5   \n 3 Nf3            725     3.61  \n 4 c4             716     3.57  \n 5 e3             416     2.07  \n 6 g3             186     0.927 \n 7 b3             173     0.862 \n 8 f4             166     0.828 \n 9 d3             131     0.653 \n10 Nc3             99     0.494 \n11 b4              88     0.439 \n12 c3              56     0.279 \n13 g4              38     0.189 \n14 h4              33     0.165 \n15 a4              28     0.140 \n16 a3              27     0.135 \n17 f3              23     0.115 \n18 Nh3             15     0.0748\n19 h3              14     0.0698\n20 Na3              4     0.0199"
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "Reverse 3-0s",
    "section": "",
    "text": "In a best-of-5 set, a reverse 3-0 occurs when one side loses the first two games, and then wins the next three to win a set. It’s an uncommon and extremely exciting (or heartbreaking) phenomenon, and it can be pretty difficult to predict when it might occur. I want to estimate how frequently this happens when a set starts 2-0, depending on the relative strength of both sides.\nThis needs some assumptions. Firstly, the chance of one player or team to win a game heavily depends on relative skill, the volatility of the game, and maybe luck. Without looking at data from a specific game, we can create chances that seem to make sense.\nWe also have to look at players/teams of a somewhat comparable skill. If you were watching a best-of-5 tournament match, and you saw one side go down 0-2, you would only wonder if they could win the match if they were capable of making it competitive. In a 1024-entrant bracket, odds are, the 1st seed will beat the 1024th seed, and the 512th seed, and the 256th seed handily in a 3-0. While those could technically be a reverse 3-0 after the first two games, it’s very unlikely and nobody would expect it to happen. We want to look at matches where some competitiveness is expected.\nWe can use a 16-player tournament, and assume that, in a vacuum, the odds of one side winning a game are the the ratio of the two teams’ seeds. For instance, seed 1 has a 1:2 chance of beating seed 2 (67%), seed 16 has a 16:1 chance of beating seed 1 (6%), and seed 15 has 15:16 odds of beating seed 16 (48%). Each of these matches would be competitive, with a significant degree of variability, which makes it interesting.\nLet’s create a hypothetical match between a random pairing of two seeds from 1-16. We can take one side’s odds of winning a game to the power of three and determine the chance of them winning three games in a row. However, since we want to know the chance of a reverse sweep rather than just a sweep, we need to account for the fact that the player or team that goes up 2-0 might have some additional advantage specifically related to that opponent: experience in the matchup, knowledge of habits or strategies, a favorable difference in skill sets, etc. Therefore, the other side’s chance of winning a game is most likely lower than initially assumed.\nHow can we account for this? If you flip a coin twice, 25% of the time, it’ll land on tails twice by pure random chance. In that spirit, let’s say that when a set starts 2-0, 75% of the time it’s because of a difference in skill or preparation, and 25% of the time it happened just because of variability with no real pattern beneath it. We can take the losing side’s initial estimated chance of winning, and reduce it by 25% to account for this factor. Based on my general experience with spectating tournaments, this is a somewhat accurate measure.\n\nlibrary(tidyverse)\nlibrary(purrr)\n\n\n# Precursor for the upset_factor() function\nexpected_placement &lt;- function(x) {\n  if (x &gt;= 1 & x &lt;= 4) {\n    order &lt;- x\n  }\n  else if (x == 5 | x == 6) {\n    order &lt;- 5\n  }\n  else if (x ==7 | x == 8) {\n    order &lt;- 6\n  }\n  else if (x &gt;= 9 & x &lt;= 12) {\n    order &lt;- 7\n  }\n  else if (x &gt;= 13 & x &lt;= 16) {\n    order &lt;- 8\n  }\n}\n\n# This scales the difference in seeds according to the difference in their expected placements in a double elimination bracket. Seed 1 vs Seed 4 is a similar difference to Seed 5 vs Seed 15. \nupset_factor &lt;- function(x, y) {\n  upset &lt;- expected_placement(x) - expected_placement(y)\n}\n\n# Randomizes seeds of two opponents, determines the odds of the second team winning games after going down 0-2, finds the seed differential from the previous functions.\nreverse_sweep_chance &lt;- function() {\n  first_two_winner &lt;- sample(1:16, 1)\n  reverse_sweep_candidate &lt;- sample(1:16, 1)\n  win_odds &lt;- (first_two_winner / (first_two_winner + reverse_sweep_candidate) * 0.75)\n  did_they_win &lt;- (runif(1) &lt; win_odds * win_odds * win_odds)\n  seed_comparison &lt;- upset_factor(reverse_sweep_candidate, first_two_winner)\n  return(data.frame(reverse_sweep = did_they_win, upset_factor = seed_comparison))\n}\n\n# Repeats the test\ndata &lt;- map(1:1000, ~ reverse_sweep_chance()) |&gt;\n  list_rbind() \n\n# Finds the overall total of reverse sweeps that happened\ndata |&gt;\n  summarize(chance = mean(reverse_sweep))\n\n  chance\n1  0.079\n\n# Plots the data\ndata |&gt;\n  ggplot(aes(x = upset_factor, fill = reverse_sweep)) +\n  geom_bar(position = 'fill') +\n  labs(\n    x = 'Upset Factor',\n    y = 'Reverse 3-0 Chance',\n    fill = 'Did It Happen?',\n    title = 'Reverse Sweep Chance Depending on Seed Differential',\n    alt = 'Bar chart showing the chance of a reverse sweep for a range of seed differentials, based on a simulation of 1000 trials. Probability decreases linearly, and ranges from about 40% for -8 to 0% for 4 and above.'\n  )\n\n\n\n\n\n\n\n\nI think this ended up being a very good estimation of the chances of this happening across different seeds. While relatively equally-skilled opponents can rarely make a reverse sweep, when the opponent that’s up 2-0 is marginally higher seeded it almost never happens. Meanwhile, when the lower seeded player goes up early, they still stand a very significant chance of getting reverse swept. This simulation largely aligns with my experience with competitive games and sports.\nThis simulation helps to illustrate how likely (or unlikely) a major comeback is, even with a high degree of volatility. Conversely, it shows how an early match lead in a best of 5 does not remotely guarantee victory against a better opponent. While the exact numbers definitely could be adjusted for more accurate results across different games, I think this does a good job of representing the probability of a reverse sweep depending on the strength of one’s opponent. Despite the fact that the odds of winning are completely made up, I do think the simulation lends credibility to the idea that the likelihood of this phenomenon can be extremely variable depending on the relative skill levels of two opponents."
  },
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "SQL: Hungry Cop Effect",
    "section": "",
    "text": "It’s been observed that judges are less likely to grant lenient sentences or parole when they are hungry. (i.e. before their lunch break) (Danziger et al., 2011). This has become known as the “hungry judge effect”.\nWe can use data from the Stanford Open Policing Project (Pierson et al., 2020) to observe the rate at which traffic stops result in citations, and whether there’s a meaningful difference in this rate before and after noon (which, presumably, is roughly when cops would eat lunch and be the hungriest). Washington, Texas, and New Jersey all give data for citations given and time of day.\nIf we assume that a standard lunch hour is 12-1pm, we can consider the prior hour (11-12) “before lunch” and the next hour (1-2) “after lunch”. While traffic stops obviously happen outside of those hours, it doesn’t make sense to evaluate the effect of a lunch break on something happening in the middle of the night, or even at 9am or 5pm."
  },
  {
    "objectID": "sql.html#introduction",
    "href": "sql.html#introduction",
    "title": "SQL: Hungry Cop Effect",
    "section": "",
    "text": "It’s been observed that judges are less likely to grant lenient sentences or parole when they are hungry. (i.e. before their lunch break) (Danziger et al., 2011). This has become known as the “hungry judge effect”.\nWe can use data from the Stanford Open Policing Project (Pierson et al., 2020) to observe the rate at which traffic stops result in citations, and whether there’s a meaningful difference in this rate before and after noon (which, presumably, is roughly when cops would eat lunch and be the hungriest). Washington, Texas, and New Jersey all give data for citations given and time of day.\nIf we assume that a standard lunch hour is 12-1pm, we can consider the prior hour (11-12) “before lunch” and the next hour (1-2) “after lunch”. While traffic stops obviously happen outside of those hours, it doesn’t make sense to evaluate the effect of a lunch break on something happening in the middle of the night, or even at 9am or 5pm."
  },
  {
    "objectID": "sql.html#data",
    "href": "sql.html#data",
    "title": "SQL: Hungry Cop Effect",
    "section": "Data",
    "text": "Data\n\nLibraries\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\n\n\nSQL Connection\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\n\nQuery\n\nSELECT 'WA' AS state,\n       CASE\n        WHEN time &gt;= '11:00:00' AND time &lt;= '12:00:00' THEN 'morning'\n        WHEN time &gt;= '13:00:00' AND time &lt;= '14:00:00' THEN 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate,\n       COUNT(*) AS total_stops\nFROM wa_statewide_2020_04_01\nWHERE time &gt;= '11:00:00' AND time &lt;= '12:00:00' OR time &gt;= '13:00:00' AND time &lt;= '14:00:00'\nGROUP BY day_segment\n\nUNION ALL\n\nSELECT 'TX' AS state,\n       CASE\n        WHEN time &gt;= '11:00:00' AND time &lt;= '12:00:00' THEN 'morning'\n        WHEN time &gt;= '13:00:00' AND time &lt;= '14:00:00' THEN 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate,\n       COUNT(*) AS total_stops\nFROM tx_statewide_2020_04_01\nWHERE time &gt;= '11:00:00' AND time &lt;= '12:00:00' OR time &gt;= '13:00:00' AND time &lt;= '14:00:00'\nGROUP BY day_segment\n\nUNION ALL\n\nSELECT 'NJ' AS state,\n       CASE\n        WHEN time &gt;= '11:00:00' AND time &lt;= '12:00:00' THEN 'morning'\n        WHEN time &gt;= '13:00:00' AND time &lt;= '14:00:00' THEN 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate,\n       COUNT(*) AS total_stops\nFROM nj_statewide_2020_04_01\nWHERE time &gt;= '11:00:00' AND time &lt;= '12:00:00' OR time &gt;= '13:00:00' AND time &lt;= '14:00:00'\nGROUP BY day_segment;\n\nThis takes the overall data for a state, filters out times outside of the normal workday, adds a column that specifies what part of the day a stop occurred at based on the time, and splits the table into two parts for each part of the day. It averages the column “citation_issued”, which is 1s for yes and 0s for no, to get the rate at which citations were issued at each part of the day. It combines this output for three different states into one table.\n\ndbDisconnect(con_traffic)\n\ncitations_and_warnings\n\n  state day_segment citation_rate warning_rate total_stops\n1    WA   afternoon     0.3985667    0.4264634     1314603\n2    WA     morning     0.4216448    0.4285162     1236714\n3    TX   afternoon     0.3789574    0.7554030     1643900\n4    TX     morning     0.3843643    0.7475949     1210867\n5    NJ   afternoon     0.6209951    0.4527666      185520\n6    NJ     morning     0.6256819    0.4511785      216405\n\n\nThe final data from the SQL query shows one row for each pairing of state and part of the day, and the proportion of traffic stops resulting in a citation or a warning in each."
  },
  {
    "objectID": "sql.html#visualization",
    "href": "sql.html#visualization",
    "title": "SQL: Hungry Cop Effect",
    "section": "Visualization",
    "text": "Visualization\n\ncitations_and_warnings |&gt;\n  ggplot() +\n  geom_col(\n    aes(x = state, y = citation_rate, fill = day_segment),\n    position = 'dodge'\n  ) +\n  labs(\n    x = 'State',\n    fill = 'Part of Day',\n    y = 'Citation Rate',\n    title = 'Frequency of Citations from Traffic Stops at Different Times of Day',\n    alt = 'Bar chart showing the frequency of citations from traffic stops from 11am to 12pm, and 1pm to 2pm, in New Jersey, Washington, and Texas. In each state, the frequency for the earlier hour is slightly higher. Overall citation rate is much higher in New Jersey.'\n  )\n\n\n\n\n\n\n\ncitations_and_warnings |&gt;\n  ggplot() +\n  geom_col(\n    aes(x = state, y = warning_rate, fill = day_segment),\n    position = 'dodge') +\n  labs(\n    x = 'State',\n    fill = 'Part of Day',\n    y = 'Warning Rate',\n    title = 'Frequency of Warnings from Traffic Stops at Different Times of Day',\n    alt = 'Bar chart showing the frequency of warnings from traffic stops from 11am to 12pm, and 1pm to 2pm, in New Jersey, Washington, and Texas. Bars for each hour are not meaningfully different. Overall warning rate is much higher in Texas.'\n  )"
  },
  {
    "objectID": "sql.html#interpretation",
    "href": "sql.html#interpretation",
    "title": "SQL: Hungry Cop Effect",
    "section": "Interpretation",
    "text": "Interpretation\nThe answer seems to be “maybe”. While 42% of morning traffic stops in Washington state result in citations, compared to 40% of afternoon ones, this pattern is much weaker in the other two states, with a difference of about half a percentage point in each. The rate at which warnings are given is essentially identical. With a sample size of over a million for Washington and Texas, it is possible that such a small difference is still statistically significant.\nNotably, without the time constraint (just observing differences between before and after 12pm), the morning citation rates are considerably higher than the afternoon ones for all three states. I assume this is because “morning” encompasses late-night traffic stops, which evidently result in much more citations.\nOverall, I don’t think these findings are conclusive. Big states like California and New Jersey do not supply time of citation information, so it may be helpful to collect more data to confirm these findings. I don’t want to rule out the possibility that such an effect exists but if it is it’s much less blatant than it is when observed in judges."
  },
  {
    "objectID": "sql.html#references",
    "href": "sql.html#references",
    "title": "SQL: Hungry Cop Effect",
    "section": "References",
    "text": "References\n\nDanziger, Shal, Jonathan Levav, Liora Avnaim-Pesso (2011). “Extraneous Factors in Judicial Decisions.” Proc. Natl. Acad. Sci. U.S.A., 108 (17) 6889-6892.\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. (2020). “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "dataviz2.html",
    "href": "dataviz2.html",
    "title": "National Park Species",
    "section": "",
    "text": "This analysis uses data from the October 8, 2024 TidyTuesday.\nThis is a dataset of species in American national parks, specifically the 15 most visited. The data is sourced from NPSpecies, a tool maintained by the National Park Service (NPS)’s Integrated Resource Management Applications (IRMA). It includes taxonomy information and other characteristics like sensitivities and rarity for each species found in each park."
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Biased Hiring",
    "section": "",
    "text": "In the mid-2010s, Amazon developed an AI-based tool trained to evaluate resumes to output the best possible candidates without the need for human review. This project failed, because it turned out the training data and the output was biased against women."
  },
  {
    "objectID": "dataviz3.html",
    "href": "dataviz3.html",
    "title": "National Park Species (Interactive)",
    "section": "",
    "text": "library(tidyverse)\n\nspecies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-08/most_visited_nps_species_data.csv')\n\nanimals &lt;- species |&gt;\n  filter(CategoryName == 'Mammal' | CategoryName == 'Fish' | CategoryName == 'Bird' | CategoryName == 'Amphibian' | CategoryName == 'Reptile')\n\nspecies &lt;- species |&gt;\n  mutate(ParkName = str_replace(ParkName, ' National Park', '')) |&gt;\n  mutate(ParkName = fct_infreq(ParkName))\n\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  fluidRow(\n    column(6,\n           selectInput(\"family\", \"Family:\", \n                       choices = sort(unique(animals$Family)),\n                       selected = \"Mustelidae\")\n    ),\n    column(6,\n           textOutput('sample')\n    )\n  ),\n  \n  plotOutput(\"shinyplot\")\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$shinyplot &lt;- renderPlot({\n    species |&gt;\n      filter(Family == input$family) |&gt;\n      ggplot(aes(y = ParkName)) +\n      geom_bar(fill = '#003f00') +\n      labs(\n        y = 'National Park',\n        x = '# Species',\n        title = paste('Distinct species in family', input$family, 'in most popular national parks')\n      ) +\n      theme_minimal()\n  })\n  \n  output$sample &lt;- renderText({\n    family_species &lt;- species |&gt;\n      filter(Family == input$family)\n    \n    paste('Example species:', sample(family_species$CommonNames, 1))\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\nhttps://nikhilshah.shinyapps.io/species/"
  },
  {
    "objectID": "dataviz2.html#introduction",
    "href": "dataviz2.html#introduction",
    "title": "National Park Species",
    "section": "",
    "text": "This analysis uses data from the October 8, 2024 TidyTuesday.\nThis is a dataset of species in American national parks, specifically the 15 most visited. The data is sourced from NPSpecies, a tool maintained by the National Park Service (NPS)’s Integrated Resource Management Applications (IRMA). It includes taxonomy information and other characteristics like sensitivities and rarity for each species found in each park."
  },
  {
    "objectID": "dataviz2.html#analysis",
    "href": "dataviz2.html#analysis",
    "title": "National Park Species",
    "section": "Analysis",
    "text": "Analysis\n\nData Import\n\nlibrary(tidyverse)\nlibrary(shiny)\n\nspecies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-08/most_visited_nps_species_data.csv')\n\n\n\nData Wrangling and Mustelid Barplot\n\nspecies &lt;- species |&gt;\n  mutate(ParkName = str_replace(ParkName, ' National Park', '')) |&gt;\n  mutate(ParkName = fct_infreq(ParkName))\n\nspecies |&gt;\n  filter(Family == 'Mustelidae') |&gt;\n  ggplot(aes(y = ParkName)) +\n  geom_bar() +\n  labs(\n    y = 'National Park',\n    x = '# Species',\n    title = 'Distinct species in family Mustelidae in 15 national parks',\n    alt = 'Bar chart showing how many species in family Mustelidae are in the fifteen most visited U.S. national parks, including Yosemite, Acadia, Grand Teton, and more. Grand Teton and Indiana Dunes have the most species, with nine apiece.'\n  )\n\n\n\n\n\n\n\n\nThis data displays the number of mustelid (weasels, badgers, otters) species in each of the 15 most visited national parks. This is because mustelids are my favorite taxon. I would have included my second-favorite taxon, pinnipeds (seals, sea lions, walruses), however I discovered there are none in any of the listed parks. The only conclusion I draw from this is that more people should visit Alaskan national parks.\nYou can edit the plot to show how prolific any mammal family is within this subset of national parks using an interactive Shiny app.\n\nShiny App Code\nInteractive"
  },
  {
    "objectID": "dataviz2.html#references",
    "href": "dataviz2.html#references",
    "title": "National Park Species",
    "section": "References",
    "text": "References\n\nTidyTuesday, National Park Species. R For Data Science, October 8, 2024.\nNPSpecies, Species List. U.S. National Park Service."
  },
  {
    "objectID": "presentation.html#project-1-data-visualization",
    "href": "presentation.html#project-1-data-visualization",
    "title": "National Park Species",
    "section": "Project 1 (Data Visualization)",
    "text": "Project 1 (Data Visualization)\n\nTidyTuesday\n10/8/24, National Park Species\nDataset: every species in every U.S. national park"
  },
  {
    "objectID": "presentation.html#initial-project",
    "href": "presentation.html#initial-project",
    "title": "National Park Species",
    "section": "Initial Project",
    "text": "Initial Project\n\nVery simple and mostly unhelpful chart of how many species of one specific animal family are in various national parks\n\n\nspecies |&gt;\n  filter(Family == 'Mustelidae') |&gt;\n  ggplot(aes(y = ParkName)) +\n  geom_bar() +\n  labs(\n    y = 'National Park',\n    x = '# Species',\n    title = 'Distinct species in family Mustelidae in 15 national parks'\n  )"
  },
  {
    "objectID": "presentation.html#updated-project",
    "href": "presentation.html#updated-project",
    "title": "National Park Species",
    "section": "Updated Project",
    "text": "Updated Project\n\nInteractive Shiny tool to control which animal taxon is displayed by the plot\nModerately more useful as you can see what types of natural spaces are primarily home to every category of animal\nShows a randomly-sampled common species name from the selected family to make it more understandable\nLink to code\nLink to app"
  },
  {
    "objectID": "presentation.html#plot",
    "href": "presentation.html#plot",
    "title": "National Park Species",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "textanalysis.html#data",
    "href": "textanalysis.html#data",
    "title": "Palindromes",
    "section": "",
    "text": "The dataset is English Words by dwyl (Github). It contains almost all of the words in the English dictionary.\n\nlibrary(tidyverse)\nlibrary(stringi)\n\nwords &lt;- read.csv(\"data/words.csv\")\n\nIt also contains many things that are decidedly not words.\n\n\n\ncategory\nexample\nthing to filter out\n\n\n\n\nhas number\n1080\ndigits\n\n\nhas symbol\nA&P\nsymbols\n\n\nproper noun\nAbdul\ncapitals\n\n\ninitialism\nA.I.\ncapitals, periods\n\n\ncontractions\naren’t\napostrophes\n\n\npossessives\nabbot’s\napostrophes\n\n\ncompounds\nable-bodied\nhyphens\n\n\n\nI ended up filtering out any word that doesn’t have a vowel from the list of palindromes as well. While there are some vowel-less words that are valid words, it’s hard to argue that “m” or “tgt” or “xxx” are among them.\n\nwords_filtered &lt;- words |&gt;\n  rename('word' = X2) |&gt;\n  filter(!str_detect(word, '[[:upper:][:punct:]\\\\d]')) |&gt;\n  mutate(length = str_length(word)) |&gt;\n  mutate(start= str_sub(word, 1, 1))\n  \nsample_n(words_filtered, 10)\n\n              word length start\n1           achime      6     a\n2        macerater      9     m\n3  nonpromulgation     15     n\n4         nicotian      8     n\n5     unvoluptuous     12     u\n6      postfrontal     11     p\n7         niteries      8     n\n8     extralegally     12     e\n9            crash      5     c\n10          unclot      6     u"
  },
  {
    "objectID": "textanalysis.html#palindromes",
    "href": "textanalysis.html#palindromes",
    "title": "Palindromes",
    "section": "Palindromes",
    "text": "Palindromes\nA palindrome is any word (or set of words) that is the same when the letters are reversed.\n\npalindromes &lt;- words_filtered |&gt;\n  filter(word == stri_reverse(word)) |&gt;\n  filter(str_detect(word, '[aeiouy]'))\n\nsample_n(palindromes, 10)\n\n     word length start\n1     eme      3     e\n2    affa      4     a\n3  terret      6     t\n4     gig      3     g\n5   torot      5     t\n6   alula      5     a\n7     yay      3     y\n8    goog      4     g\n9     tet      3     t\n10    awa      3     a\n\n\nA lot of palindromes are formed because of common suffixes. ‘Deified’, ‘reviver’, and ‘stats’ might be palindromes, but only because of a modifier that changes the root word. This shows how many words follow those structures.\n\npalindromes &lt;- palindromes |&gt;\n  mutate(suffix = ifelse(str_detect(word, \"\\\\b(de|re|ro|s)\\\\w+(ed|er|or|s)\\\\b\"), 'Yes', 'No'))\n\npalindromes |&gt;\n  ggplot(aes(fill = suffix, x = length)) +\n  geom_bar() +\n  labs(\n    x = 'Word Length',\n    y = '# of Palindromes',\n    title = 'Single-Word Palindromes in English',\n    alt = 'Bar chart showing the word lengths of single-word palindromes in English. Bars are separated into two sections, with the minority of most columns indicating that they end in a common suffix such as an s. Words are concentrated from length 3-7, mainly 3 and 5.'\n  )\n\nThere are only 118 palindromes in this dataset. Of these, there’s only one palindrome longer than seven letters, and 29 are only palindromes because of a suffix, and the root word wouldn’t be one at all. In total, there are about 30 of them that you might encounter in your day-to-day life, which is only 1% of 1% of the words in English. I hear about palindromes constantly, but it turns out most of them are actually just engineered phrases like “race car” or “a man, a plan, a canal, panama”.\nI think it’s interesting how there seems to be some kind of inclination towards odd-length words, that have a unique letter in the middle, like “civic” or “refer” or “tenet”.\n\nwords_filtered |&gt;\n  mutate(length_type = ifelse(length %% 2 == 0, 'even', 'odd')) |&gt;\n  ggplot(aes(x = length_type)) +\n  geom_bar() +\n  labs(\n    x = 'Even or Odd?',\n    y = '# of Words',\n    title = 'Lengths of English Words',\n    alt = 'Simple bar chart showing how many English words are an odd or even length. Very slightly more are even.'\n  )\n\n\n\n\n\n\n\n\nI tend to think of a syllable as a pair of a consonant sound and a vowel sound, and there’s a slight amount more of even-length words than odd in English, so I’m a bit surprised to see such a preference for odd-length ones out of the palindromes."
  },
  {
    "objectID": "textanalysis.html#anti-palindromes-reduplications",
    "href": "textanalysis.html#anti-palindromes-reduplications",
    "title": "Palindromes",
    "section": "Anti-Palindromes (Reduplications)",
    "text": "Anti-Palindromes (Reduplications)\nWhile trying to work out a way to find palindromes with stringr functions (apparently R does not have reverse indexing like Python does so this is either not possible or very convoluted) I accidentally came up with a list of “anti-palindromes”, where the back half of the word is also the front half of the word. If “toot” is a palindrome, “toto” is an anti-palindrome.\n\nreduplications &lt;- words_filtered |&gt; \n  filter(str_sub(word, 1, floor(str_length(word)/2)) == str_sub(word, -ceiling(str_length(word)/2), -1))\n\nwords_letters &lt;- words_filtered |&gt;\n  group_by(start) |&gt;\n  summarize(count_total = n()) |&gt;\n  mutate(\"All English Words\" = count_total/sum(count_total)*100)\n\nreduplication_letters &lt;- reduplications |&gt;\n  group_by(start) |&gt;\n  summarize(count_red = n()) |&gt;\n  mutate(\"Anti-Palindromes\" = count_red/sum(count_red)*100)\n\nletters &lt;- words_letters |&gt;\n  left_join(reduplication_letters)\n\nJoining with `by = join_by(start)`\n\nletters_long &lt;- letters |&gt;\n  pivot_longer(cols = c(\"All English Words\", \"Anti-Palindromes\"), names_to = 'dictionary', values_to = 'percent') \n\nletters_long |&gt;\n  ggplot(aes(fill = dictionary, x = start, y = percent)) +\n  geom_bar(position = 'dodge', stat = 'identity') +\n  labs(\n    x = 'Letter',\n    y = 'Frequency (Percentage of Overall Set)',\n    title = 'Starting Letters of Anti-Palindromes',\n    alt = 'Bar chart showing the frequency of starting letters for anti-palindromes, as well as the English language as a whole, ordered alphabetically. Highest for English are S, P, C, A, U. Highest for anti-palindromes are T, K, M, G, B, P.'\n  )\n\n\n\n\n\n\n\n\nInterestingly, a lot of these anti-palindromes, or reduplications, start with uncommon letters. This could because many of those letters are emphatic consonant sounds that sound more interesting when repeated. Examples include beriberi, yoyo, and pompom."
  },
  {
    "objectID": "ethics.html#introduction",
    "href": "ethics.html#introduction",
    "title": "Biased Hiring",
    "section": "",
    "text": "In the mid-2010s, Amazon developed an AI-based tool trained to evaluate resumes to output the best possible candidates without the need for human review. This project failed, because it turned out the training data and the output was biased against women."
  },
  {
    "objectID": "ethics.html#analysis",
    "href": "ethics.html#analysis",
    "title": "Biased Hiring",
    "section": "Analysis",
    "text": "Analysis\nThis failure stems from the lack of adherence to one of the principles of data science, which suggests to “build teams with diverse ideas, backgrounds, and strengths.” According to the ACLU, Amazon’s workforce was predominantly male, which led to the subset of “successful resumes” being predominantly male, so the algorithm learned that characteristics that could have been associated with women were not favorable (Goodman, 2018). Notably, the Reuters report claims that the bias was primarily due to disproportionate gender ratios in the applications themselves, rather than the existing workforce (Dastin, 2018), but it’s fair to assume that a skewed workforce would perpetuate that ratio and so the premise is essentially the same - a lack of diversity in Amazon led to a discriminatory hiring algorithm.\nAnother principle for which the project falls short is that it does not use data to improve lives. In fact, pioneering a discriminatory technology that would go on to be implemented on massive scales ten years later, governing a major factor in peoples’ quality of life (their employment), most likely played a significant role in making lives worse, even if the algorithm itself was scrapped. In the 1970s, IBM stated that “a computer can never be held accountable, therefore a computer must never make a management decision.” While collecting data for the sake of knowledege is okay, a machine-learning algorithm should not be given the power to have a tangible negative effect on human lives. Going through with this project without taking great care to prevent bias in the algorithm’s training data and thus its output may have contributed to this ideal being violated.\nI think it’s generally okay for gender to be used as a variable for data analysis purposes specifically. When collecting data on people, and that data is actionable, such as data collected on social media platforms for targeted advertising, or data collected by companies for hiring, collecting data on social categories like gender can very easily be intrusive and unethical, with the potential to be misused. However, in studies, especially when there’s a potential connection between those social categories and another variable, it can be very important to collect that data. If gender data had not been collected in this case, there would be no way of knowing about the discrimination that was occurring. Overall, the distinction probably lies in whether gender as a variable is pertinent to the project, and whether that data can be used to cause harm to the participants.\nData collection and processing in this case was most likely not conducted ethically. While the training data could have been gathered consensually (since it was from employees of the company), the project was conducted behind closed doors; actual applicants weren’t aware that their applications were being evaluated by a machine-learning algorithm. They had no way of knowing that their information was being used in a computer program which could discriminate against them for factors out of their control. On a few applications for jobs and internships, I’ve been asked whether or not I’d like to opt out of an AI evaluation of my application. While I did opt out, I imagine some or most other companies aren’t giving me the option because that was their default option. This worries me - how do I know my resume isn’t being used to train ChatGPT to hire people based on some immutable characteristic? Not only would this project have been unethical, it could have set a standard that green-lighted the same unethical behavior for broad use in the future."
  },
  {
    "objectID": "ethics.html#references",
    "href": "ethics.html#references",
    "title": "Biased Hiring",
    "section": "References",
    "text": "References\n\nDastin, Jeffrey (2018). “Amazon scraps secret AI recruiting tool that showed bias against women.” Reuters.\nGoodman, Rachel (2018). “Why Amazon’s Automated Hiring Tool Discriminated Against Women.” ACLU."
  }
]