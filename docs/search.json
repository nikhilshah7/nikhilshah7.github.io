[
  {
    "objectID": "tuesday2.html",
    "href": "tuesday2.html",
    "title": "National Park Species",
    "section": "",
    "text": "This analysis uses data from the October 8, 2024 TidyTuesday.\nThis is a dataset of species in American national parks, specifically the 15 most visited. The data is sourced from NPSpecies, a tool maintained by the National Park Service (NPS)’s Integrated Resource Management Applications (IRMA). It includes taxonomy information and other characteristics like sensitivities and rarity for each species found in each park.\n\nspecies &lt;- species |&gt;\n  filter(Family == 'Mustelidae') |&gt;\n  mutate(ParkName = str_replace(ParkName, ' National Park', '')) |&gt;\n  mutate(ParkName = fct_infreq(ParkName))\n\nspecies |&gt;\n  ggplot() +\n  geom_bar(aes(y = ParkName)) +\n  labs(\n    x = 'National Park',\n    y = '# Species',\n    title = 'Distinct species in family Mustelidae 15 national parks'\n  )\n\n\n\n\n\n\n\n\nThis data displays the number of mustelid (weasels, badgers, otters) species in each of the 15 most visited national parks. This is because mustelids are my favorite taxon. I would have included my second-favorite taxon, pinnipeds (seals, sea lions, walruses), however I discovered there are none in any of the listed parks. The only conclusion I draw from this is that more people should visit Alaskan national parks.\n\n\n# A tibble: 15 × 2\n   ParkName              count\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Glacier                   9\n 2 Grand Teton               9\n 3 Yellowstone               8\n 4 Yosemite                  8\n 5 Acadia                    7\n 6 Great Smoky Mountains     7\n 7 Olympic                   7\n 8 Rocky Mountain            7\n 9 Indiana Dunes             6\n10 Cuyahoga Valley           5\n11 Grand Canyon              4\n12 Bryce Canyon              3\n13 Hot Springs               3\n14 Zion                      3\n15 Joshua Tree               2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Palindromes",
    "section": "",
    "text": "Data\nThe dataset is English Words by dwyl (Github). It contains almost all of the words in the English dictionary.\n\nlibrary(tidyverse)\nlibrary(stringi)\n\nwords &lt;- read.csv(\"data/words.csv\")\n\nIt also contains many things that are decidedly not words.\n\n\n\ncategory\nexample\nthing to filter out\n\n\n\n\nhas number\n1080\ndigits\n\n\nhas symbol\nA&P\nsymbols\n\n\nproper noun\nAbdul\ncapitals\n\n\ninitialism\nA.I.\ncapitals, periods\n\n\ncontractions\naren’t\napostrophes\n\n\npossessives\nabbot’s\napostrophes\n\n\ncompounds\nable-bodied\nhyphens\n\n\n\nI ended up filtering out any word that doesn’t have a vowel from the list of palindromes as well. While there are some vowel-less words that are valid words, it’s hard to argue that “m” or “tgt” or “xxx” are among them.\n\nwords_filtered &lt;- words |&gt;\n  rename('word' = X2) |&gt;\n  filter(!str_detect(word, '[[:upper:][:punct:]\\\\d]')) |&gt;\n  mutate(length = str_length(word)) |&gt;\n  mutate(start= str_sub(word, 1, 1))\n  \nsample_n(words_filtered, 10)\n\n            word length start\n1  ventriloquous     13     v\n2     microjoule     10     m\n3         siglum      6     s\n4      halfblood      9     h\n5      keyboards      9     k\n6        kinepox      7     k\n7       sacculus      8     s\n8   undersurface     12     u\n9         trolls      6     t\n10     unamusing      9     u\n\n\n\n\nPalindromes\nA palindrome is any word (or set of words) that is the same when the letters are reversed.\n\npalindromes &lt;- words_filtered |&gt;\n  filter(word == stri_reverse(word)) |&gt;\n  filter(str_detect(word, '[aeiouy]'))\n\nsample_n(palindromes, 10)\n\n     word length start\n1  degged      6     d\n2     ese      3     e\n3    toot      4     t\n4  hallah      6     h\n5    teet      4     t\n6   tipit      5     t\n7     mom      3     m\n8     hah      3     h\n9  tirrit      6     t\n10   ecce      4     e\n\n\nA lot of palindromes are formed because of common suffixes. ‘Deified’, ‘reviver’, and ‘stats’ might be palindromes, but only because of a modifier that changes the root word. This shows how many words follow those structures.\n\npalindromes &lt;- palindromes |&gt;\n  mutate(suffix = ifelse(str_detect(word, \"\\\\b(de|re|ro|s)\\\\w+(ed|er|or|s)\\\\b\"), 'Yes', 'No'))\n\npalindromes |&gt;\n  ggplot(aes(fill = suffix, x = length)) +\n  geom_bar() +\n  labs(\n    x = 'Word Length',\n    y = '# of Palindromes',\n    title = 'Single-Word Palindromes in English'\n  )\n\nThere are only 118 palindromes in this dataset. Of these, there’s only one palindrome longer than seven letters, and 29 are only palindromes because of a suffix, and the root word wouldn’t be one at all. In total, there are about 30 of them that you might encounter in your day-to-day life, which is only 1% of 1% of the words in English. I hear about palindromes constantly, but it turns out most of them are actually just engineered phrases like “race car” or “a man, a plan, a canal, panama”.\nI think it’s interesting how there seems to be some kind of inclination towards odd-length words, that have a unique letter in the middle, like “civic” or “refer” or “tenet”.\n\nwords_filtered |&gt;\n  ggplot(aes(x = length)) +\n  geom_bar() +\n  labs(\n    x = 'Word Length',\n    y = '# of Words',\n    title = 'Length of English Words'\n  )\n\n\n\n\n\n\n\n\nI tend to think of a syllable as a pair of a consonant sound and a vowel sound, and it looks like there’s a slight amount more of even-length words than odd in English, so I’m a bit surprised to see such a preference for odd-length ones out of the palindromes.\n\n\nAnti-Palindromes, or Reduplications\nWhile trying to work out a way to find palindromes with stringr functions (apparently R does not have reverse indexing like Python does so this is either not possible or very convoluted) I accidentally came up with a list of “anti-palindromes”, where the back half of the word is also the front half of the word. If “toot” is a palindrome, “toto” is an anti-palindrome.\n\nreduplications &lt;- words_filtered |&gt; \n  filter(str_sub(word, 1, floor(str_length(word)/2)) == str_sub(word, -ceiling(str_length(word)/2), -1))\n\nwords_letters &lt;- words_filtered |&gt;\n  group_by(start) |&gt;\n  summarize(count_total = n()) |&gt;\n  mutate(\"All English Words\" = count_total/sum(count_total)*100)\n\nreduplication_letters &lt;- reduplications |&gt;\n  group_by(start) |&gt;\n  summarize(count_red = n()) |&gt;\n  mutate(\"Anti-Palindromes\" = count_red/sum(count_red)*100)\n\nletters &lt;- words_letters |&gt;\n  left_join(reduplication_letters)\n\nJoining with `by = join_by(start)`\n\nletters_long &lt;- letters |&gt;\n  pivot_longer(cols = c(\"All English Words\", \"Anti-Palindromes\"), names_to = 'dictionary', values_to = 'percent') \n\nletters_long |&gt;\n  ggplot(aes(fill = dictionary, x = start, y = percent)) +\n  geom_bar(position = 'dodge', stat = 'identity') +\n  labs(\n    x = 'Letter',\n    y = 'Frequency (%)',\n    title = 'Starting Letters of Anti-Palindromes'\n  )\n\n\n\n\n\n\n\n\nInterestingly, a lot of these anti-palindromes, or reduplications, start with uncommon letters. This could because many of those letters are emphatic consonant sounds that sound more interesting when repeated. Examples include beriberi, yoyo, and pompom."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nikhil Shah",
    "section": "",
    "text": "This is my website. It currently contains two data visualization exercises."
  },
  {
    "objectID": "tuesday1.html",
    "href": "tuesday1.html",
    "title": "Lichess.org",
    "section": "",
    "text": "This analysis uses data from the October 1, 2024 TidyTuesday.\nThis is a dataset of 20,058 online chess games. The data was collected and cleaned from Lichess.org by Kaggle/Mitchell J. It includes extensive data on each game including each move used, both players’ usernames and ratings, and the time of the game.\n\n# adds a column that isolates the opening move of each game, and orders it by how common each move is\nchess &lt;- chess |&gt;\n  mutate(\n    opening_move = ifelse(\n      str_sub(moves, 1, 1)=='N', \n      str_sub(moves, 1, 3), \n      str_sub(moves, 1, 2)\n      )\n    ) |&gt;\n  mutate(\n    opening_move = fct_infreq(opening_move)\n    )\n\n# creates a bar plot that shows the frequency of opening moves\nchess |&gt;\n  ggplot() +\n  geom_bar(aes(x = opening_move)) +\n  labs(\n    x = 'Opening Move',\n    y = 'Number of Games',\n    title = 'Frequency of opening moves for white on Lichess.com'\n  )\n\n\n\n\n\n\n\n\nThis bar plot shows the frequency of initial moves in the dataset. e4 and d4 make up almost all of them. While knight to f3 is the third most popular, almost none of the games use knight to a3.\n\n\n# A tibble: 20 × 3\n   opening_move count proportion\n   &lt;fct&gt;        &lt;int&gt;      &lt;dbl&gt;\n 1 e4           12598    62.8   \n 2 d4            4522    22.5   \n 3 Nf3            725     3.61  \n 4 c4             716     3.57  \n 5 e3             416     2.07  \n 6 g3             186     0.927 \n 7 b3             173     0.862 \n 8 f4             166     0.828 \n 9 d3             131     0.653 \n10 Nc3             99     0.494 \n11 b4              88     0.439 \n12 c3              56     0.279 \n13 g4              38     0.189 \n14 h4              33     0.165 \n15 a4              28     0.140 \n16 a3              27     0.135 \n17 f3              23     0.115 \n18 Nh3             15     0.0748\n19 h3              14     0.0698\n20 Na3              4     0.0199"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Reverse 3-0 Odds",
    "section": "",
    "text": "In a best-of-5 set, a reverse 3-0 occurs when one side loses the first two games, and then wins the next three to win a set. It’s an uncommon and extremely exciting (or heartbreaking) phenomenon, and it can seem to often much more likely or much less often than you would expect. I want to estimate how frequently this happens when a set starts 2-0, depending on the relative strength of both sides.\nThis needs some assumptions. Firstly, the odds of one player or team to win a game heavily depends on relative skill, the volatility of the game, and maybe luck. Without looking at data from a specific game, we can create odds that seem to make sense.\nWe also have to look at players/teams of a somewhat comparable skill. If you were watching a best-of-5 tournament match, and you saw one side go down 0-2, you would only wonder if they could win the match if they were capable of making it competitive. In a 1024-entrant bracket, odds are, the 1st seed will beat the 1024th seed, and the 512th seed, and the 256th seed handily in a 3-0. While those could technically be a reverse 3-0 after the first two games, it’s very unlikely and nobody would expect it to happen. We want to look at matches where some competitiveness is expected.\nWe can use a 16-player tournament, and assume that, in a vacuum, the odds of one side winning a game are the the ratio of the two teams’ seeds. For instance, seed 1 has a 1:2 chance of beating seed 2 (67%), seed 16 has a 16:1 chance of beating seed 1 (6%), and seed 15 has 15:16 odds of beating seed 16 (48%). Each of these matches would be competitive, with a significant degree of variability, which makes it interesting.\nLet’s create a hypothetical match between a random pairing of two seeds from 1-16. We can take one side’s odds of winning a game to the power of three and determine the chance of them winning three games in a row. However, since we want to know the chance of a reverse sweep rather than just a sweep, we need to account for the fact that the player or team that goes up 2-0 might have some additional advantage specifically related to that opponent: experience in the matchup, knowledge of habits or strategies, a favorable difference in skill sets, etc. Therefore, the other side’s chance of winning a game is most likely lower than initially assumed.\nHow can we account for this? If you flip a coin twice, 25% of the time, it’ll land on tails twice by pure random chance. In that spirit, let’s say there was a 25% chance that the 2-0 start was purely based on random variability, and reduce the losing side’s new odds of winning a game by 25%. Based on my general experience with spectating tournaments, this is a somewhat accurate measure.\n\nlibrary(tidyverse)\nlibrary(purrr)\n\n\n# Precursor for the upset_factor() function\nexpected_placement &lt;- function(x) {\n  if (x &gt;= 1 & x &lt;= 4) {\n    order &lt;- x\n  }\n  else if (x == 5 | x == 6) {\n    order &lt;- 5\n  }\n  else if (x ==7 | x == 8) {\n    order &lt;- 6\n  }\n  else if (x &gt;= 9 & x &lt;= 12) {\n    order &lt;- 7\n  }\n  else if (x &gt;= 13 & x &lt;= 16) {\n    order &lt;- 8\n  }\n}\n\n# This scales the difference in seeds according to the difference in their expected placements in a double elimination bracket. Seed 1 vs Seed 4 is a similar difference to Seed 5 vs Seed 15. \nupset_factor &lt;- function(x, y) {\n  upset &lt;- expected_placement(x) - expected_placement(y)\n}\n\n# Randomizes seeds of two opponents, determines the odds of the second team winning games after going down 0-2, finds the seed differential from the previous functions.\nreverse_sweep_chance &lt;- function() {\n  first_two_winner &lt;- sample(1:16, 1)\n  reverse_sweep_candidate &lt;- sample(1:16, 1)\n  win_odds &lt;- (first_two_winner / (first_two_winner + reverse_sweep_candidate) * 0.75)\n  did_they_win &lt;- (runif(1) &lt; win_odds * win_odds * win_odds)\n  seed_comparison &lt;- upset_factor(reverse_sweep_candidate, first_two_winner)\n  return(data.frame(reverse_sweep = did_they_win, upset_factor = seed_comparison))\n}\n\n# Repeats the test\ndata &lt;- map(1:1000, ~ reverse_sweep_chance()) |&gt;\n  list_rbind() \n\n# Finds the overall total of reverse sweeps that happened\ndata |&gt;\n  summarize(chance = mean(reverse_sweep))\n\n  chance\n1  0.074\n\n# Plots the data\ndata |&gt;\n  ggplot(aes(x = upset_factor, fill = reverse_sweep)) +\n  geom_bar(position = 'fill') +\n  labs(\n    x = 'Upset Factor',\n    y = 'Reverse 3-0 Chance',\n    fill = 'Did It Happen?',\n    title = 'Reverse Sweep Odds Depending on Seed Differential'\n  )\n\n\n\n\n\n\n\n\nI think this ended up being a very good estimation of the chances of this happening across different seeds. While relatively equally-skilled opponents can rarely make a reverse sweep, when the opponent that’s up 2-0 is marginally higher seeded it almost never happens. Meanwhile, when the lower seeded player goes up early, they still stand a very significant chance of getting reverse swept. This simulation largely aligns with my experience with competitive games and sports.\nThis simulation helps to illustrate how likely (or unlikely) a major comeback is, even with a high degree of volatility. Conversely, it shows how an early match lead in a best of 5 does not remotely guarantee victory against a better opponent. While the exact numbers definitely could be adjusted for more accurate results across different games, I think this does a good job of representing the probability of a reverse sweep depending on the strength of one’s opponent. Despite the fact that the odds of winning are completely made up, I do think the simulation lends credibility to the idea that the likelihood of this phenomenon can be extremely variable depending on the relative skill levels of two opponents."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Biased Hiring",
    "section": "",
    "text": "Introduction\nIn the mid-2010s, Amazon developed an AI-based tool trained to evaluate resumes to output the best possible candidates without the need for human review. This project failed, because it turned out the training data and the output was biased against women.\n\n\nAnalysis\nThis failure stems from the lack of adherence to one of the principles of data science, which suggests to “build teams with diverse ideas, backgrounds, and strengths.” According to the ACLU, Amazon’s workforce was predominantly male, which led to the subset of “successful resumes” being predominantly male, so the algorithm learned that characteristics that could have been associated with women were not favorable. Notably, the Reuters report claims that the bias was primarily due to disproportionate gender ratios in the applications themselves, rather than the existing workforce, but it’s fair to assume that a skewed workforce would perpetuate that ratio and so the premise is essentially the same - a lack of diversity in Amazon led to a discriminatory hiring algorithm.\nAnother principle for which the project falls short is that it does not use data to improve lives. In fact, pioneering a discriminatory technology that would go on to be implemented on massive scales ten years later, governing a major factor in peoples’ quality of life (their employment), most likely played a significant role in making lives worse, even if the algorithm itself was scrapped. In the 1970s, IBM stated that “a computer can never be held accountable, therefore a computer must never make a management decision.” While collecting data for the sake of knowledege is okay, a machine-learning algorithm should not be given the power to have a tangible negative effect on human lives. Going through with this project without taking great care to prevent bias in the algorithm’s training data and thus its output may have contributed to this ideal being violated.\nI think it’s generally okay for gender to be used as a variable for data analysis purposes specifically. When collecting data on people, and that data is actionable, such as data collected on social media platforms for targeted advertising, or data collected by companies for hiring, collecting data on social categories like gender can very easily be intrusive and unethical, with the potential to be misused. However, in studies, especially when there’s a potential connection between those social categories and another variable, it can be very important to collect that data. If gender data had not been collected in this case, there would be no way of knowing about the discrimination that was occurring. Overall, the distinction probably lies in whether gender as a variable is pertinent to the project, and whether that data can be used to cause harm to the participants.\nData collection and processing in this case was most likely not conducted ethically. While the training data could have been gathered consensually (since it was from employees of the company), the project was conducted behind closed doors; actual applicants weren’t aware that their applications were being evaluated by a machine-learning algorithm. They had no way of knowing that their information was being used in a computer program which could discriminate against them for factors out of their control. On a few applications for jobs and internships, I’ve been asked whether or not I’d like to opt out of an AI evaluation of my application. While I did opt out, I imagine some or most other companies aren’t giving me the option because that was their default option. This worries me - how do I know my resume isn’t being used to train ChatGPT to hire people based on some immutable characteristic? Not only would this project have been unethical, it could have set a standard that green-lighted the same unethical behavior for broad use in the future.\n\n\nSources\n\nDastin, Jeffrey (2018). “Amazon scraps secret AI recruiting tool that showed bias against women.” Reuters.\nGoodman, Rachel (2018). “Why Amazon’s Automated Hiring Tool Discriminated Against Women.” ACLU."
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "SQL: Hungry Cop Effect",
    "section": "",
    "text": "It’s been observed that judges are less likely to grant lenient sentences or parole when they are hungry. (i.e. before their lunch break) (Danziger et al., 2011). This has become known as the “hungry judge effect”.\nWe can use data from the Stanford Open Policing Project (Pierson et al., 2020) to observe the rate at which traffic stops result in citations, and whether there’s a meaningful difference in this rate before and after noon (which, presumably, is roughly when cops would eat lunch and be the hungriest). Washington, Texas, and New Jersey all give data for citations given and time of day.\nI cut off the data according to the normal 9-5 work day. While traffic stops obviously happen outside of those hours, it doesn’t make sense to evaluate the effect of a lunch break on something happening in the middle of the night."
  },
  {
    "objectID": "project5.html#introduction",
    "href": "project5.html#introduction",
    "title": "SQL: Hungry Cop Effect",
    "section": "",
    "text": "It’s been observed that judges are less likely to grant lenient sentences or parole when they are hungry. (i.e. before their lunch break) (Danziger et al., 2011). This has become known as the “hungry judge effect”.\nWe can use data from the Stanford Open Policing Project (Pierson et al., 2020) to observe the rate at which traffic stops result in citations, and whether there’s a meaningful difference in this rate before and after noon (which, presumably, is roughly when cops would eat lunch and be the hungriest). Washington, Texas, and New Jersey all give data for citations given and time of day.\nI cut off the data according to the normal 9-5 work day. While traffic stops obviously happen outside of those hours, it doesn’t make sense to evaluate the effect of a lunch break on something happening in the middle of the night."
  },
  {
    "objectID": "project5.html#data",
    "href": "project5.html#data",
    "title": "SQL: Hungry Cop Effect",
    "section": "Data",
    "text": "Data\n\nLibraries\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\n\n\nSQL Connection\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\n\nQuery\n\nSELECT 'WA' AS state,\n       CASE\n        WHEN time &lt; '12:00:00' THEN 'morning'\n        ELSE 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate\nFROM wa_statewide_2020_04_01\nWHERE time &gt; '09:00:00' AND time &lt; '17:00:00'\nGROUP BY day_segment\n\nUNION ALL\n\nSELECT 'TX' AS state,\n       CASE\n        WHEN time &lt; '12:00:00' THEN 'morning'\n        ELSE 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate\nFROM tx_statewide_2020_04_01\nWHERE time &gt; '09:00:00' AND time &lt; '17:00:00'\nGROUP BY day_segment\n\nUNION ALL\n\nSELECT 'NJ' AS state,\n       CASE\n        WHEN time &lt; '12:00:00' THEN 'morning'\n        ELSE 'afternoon'\n       END AS day_segment,\n       AVG(citation_issued) AS citation_rate,\n       AVG(warning_issued) AS warning_rate\nFROM nj_statewide_2020_04_01\nWHERE time &gt; '09:00:00' AND time &lt; '17:00:00'\nGROUP BY day_segment;"
  },
  {
    "objectID": "project5.html#visualization",
    "href": "project5.html#visualization",
    "title": "SQL: Hungry Cop Effect",
    "section": "Visualization",
    "text": "Visualization\n\ncitations_and_warnings |&gt;\n  ggplot() +\n  geom_col(\n    aes(x = state, y = citation_rate, fill = day_segment),\n    position = 'dodge'\n  ) +\n  labs(\n    x = 'State',\n    fill = 'Part of Day',\n    y = 'Citation Rate',\n    title = 'Frequency of Citations from Traffic Stops at Different Times of Day'\n  )\n\n\n\n\n\n\n\ncitations_and_warnings |&gt;\n  ggplot() +\n  geom_col(\n    aes(x = state, y = warning_rate, fill = day_segment),\n    position = 'dodge') +\n  labs(\n    x = 'State',\n    fill = 'Part of Day',\n    y = 'Warning Rate',\n    title = 'Frequency of Warnings from Traffic Stops at Different Times of Day'\n  )"
  },
  {
    "objectID": "project5.html#interpretation",
    "href": "project5.html#interpretation",
    "title": "SQL: Hungry Cop Effect",
    "section": "Interpretation",
    "text": "Interpretation\nThe answer seems to be a solid no. While 44% of morning traffic stops in Washington state result in citations, compared to 38% of afternoon ones, this pattern doesn’t exist in the other two states. The rate at which warnings are given is identical.\nNotably, without the time constraint, the morning citation rates are considerably higher than the afternoon ones for all three states. I assume this is because “morning” encompasses late-night traffic stops, which evidently result in much more citations. This proves that removing those parts of the data makes sense, because, as stated earlier, a DUI citation at 3 am probably doesn’t have any degree of dependence on whether the officer has had lunch yet.\nOverall, the null hypothesis seems to be supported in this case. Big states like California and New Jersey do not supply time of citation information, so it may be helpful to collect more data to confirm these findings."
  },
  {
    "objectID": "project5.html#references",
    "href": "project5.html#references",
    "title": "SQL: Hungry Cop Effect",
    "section": "References",
    "text": "References\n\nDanziger, Shal, Jonathan Levav, Liora Avnaim-Pesso. 2011. “Extraneous Factors in Judicial Decisions.” Proc. Natl. Acad. Sci. U.S.A., 108 (17) 6889-6892.\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  }
]